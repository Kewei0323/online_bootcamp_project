---
title: "H1-B Visa Petitions for Data Science Positions in 2015"
author: "Sharan Naribole"
output: 
  html_notebook: default
  pdf_document: default
---

Contributed by Sharan Naribole. He is currently undertaking the part-time online bootcamp organized by NYC Data Science Academy (Dec 2016- April 2017). This blog is based on his bootcamp project - R Exploratory Data Analysis

<h2> Abstract </h2>

The H1-B is an employment-based, non-immigrant visa category for temporary foreign workers in the United States. Every year, the US immigration department accepts over 200,000 petitions and selects 85,000 applications through a random process. The application data is available for public access to perform in-depth longitudinal research and analysis. This data provides key insights into the prevailing wages for job titles being sponsored by US employers under H1-B visa category. In particular, I utilize the 2015 H1-B petition disclosure data to analyze the Salary distribution across different industries, states  and seniority levels for Data Science positions. 

Let's begin by loading R packages.

```{r}
library(dplyr)
library(ggplot2)
library(zipcode)
library(readxl)
```

<h2> H1-B Visa Data Introduction </h2>
The H1-B is an employment-based, non-immigrant visa category for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, an US employer must offer a job and petition for H1-B visa with the US immigration department. This is the most common visa status applied for and held by international students once they complete college/ higher education (Masters, PhD) and work in a full-time position.

The Office of Foreign Labor Certification (OFLC) generates program data that is useful information about the immigration programs including the H1-B visa. The disclosure data updated annually is available at https://www.foreignlaborcert.doleta.gov/performancedata.cfm

Loading H1-B Visa data for 2015 downloaded from 
```{r}
h1b_data = read_excel("data/H-1B_Disclosure_Data_FY15_Q4.xlsx")
```
Let's explore the columns in our data
```{r}
colnames(h1b_data)
```

The useful columns for our data analysis include:

1) <b> EMPLOYER_NAME </b>: Name of employer submitting the H1-B application.

2) <b> JOB_TITLE </b>: Title of the job using which we can filter the Data Science positions and the Seniority Level

3) <b> SOC_NAME </b> The broad area/industry associated with a job as classified by the Standard Occupational (SOC) System. This gives us insight into the fields in which Data Scientist positions are being offered.

4) <b> PREVAILING_WAGE </b> The prevailing wage for a job position is defined as the average wage paid to similarly employed workers in the requested occupation in the area of intended employment. The prevailing wage is based on the employerâ€™s minimum requirements for the position. (<i> Source: </i> https://www.usavisanow.com/h-1b-visa/h1b-visa-resources/prevailing-wage/). This column will be our key metric in the data analysis.

5) <b> WORKSITE_CITY, WORKSITE_STATE </b> The foreign worker's intended area of employment. We will explore the relationship between prevailing wage for Data Scientist position across different locations.

I focus on the annual prevailing wage for full-time positions. Consequently, the rows confirming full_time positions are filtered.

```{r}
h1b_df = h1b_data %>% filter(FULL_TIME_POSITION == 'Y')
```
The column PW_UNIT_OF_PAY indicates the frequency of payment. As shown below, the PW_UNIT_OF_PAY varies from hour to Year. 
```{r}
h1b_df %>% select(PW_UNIT_OF_PAY ) %>% sapply(function(x) unique(x))
```

First, the rows with missing values for PW_UNIT_OF_PAY need to be removed. Then, we convert wage of remaining rows to annual scale.

```{r}
h1b_df = h1b_df %>% filter(!is.na(PW_UNIT_OF_PAY))
h1b_df = h1b_df %>% mutate(PREVAILING_WAGE = as.numeric(PREVAILING_WAGE))
```
```{r}
#Function to transform wage to annual scale
pw_unit_to_yearly = function(prevailing_wage, pw_unit_of_pay) {
  return(ifelse(pw_unit_of_pay == "Year", prevailing_wage, ifelse(pw_unit_of_pay == "Hour", 2080*prevailing_wage, ifelse(pw_unit_of_pay== "Week", 52*prevailing_wage, ifelse(pw_unit_of_pay == "Month", 12*prevailing_wage, 26*prevailing_wage)))))
}

h1b_df = h1b_df %>% mutate(PREVAILING_WAGE = pw_unit_to_yearly(PREVAILING_WAGE, PW_UNIT_OF_PAY))
h1b_df = h1b_df %>% mutate(PW_UNIT_OF_PAY = "Year")
```

Next step is to classify Jobs based on the seniority of the position. This is because seniority results in a higher wage. For example, a Senior/ Lead Data Scientist is expected to earn more than a regular Data Scientist.

```{r}
h1b_df = h1b_df %>% mutate(lead = ifelse(regexpr('lead', tolower(JOB_TITLE)) != -1 | regexpr('senior', tolower(JOB_TITLE)) != -1 | regexpr('sr.', tolower(JOB_TITLE)) != -1, 1, 0))

h1b_df = h1b_df %>% mutate(manager = ifelse(regexpr('manager', tolower(JOB_TITLE)) != -1 | regexpr('director', tolower(JOB_TITLE)) != -1 | regexpr('mgr', tolower(JOB_TITLE)) != -1 | regexpr('principal', tolower(JOB_TITLE)) != -1, 1, 0))

h1b_df = h1b_df %>% mutate(data_scientist = ifelse(regexpr('data scientist', tolower(JOB_TITLE)) != -1, 1, 0))

h1b_df = h1b_df %>% mutate(data_analyst = ifelse(regexpr('data analyst', tolower(JOB_TITLE)) != -1, 1, 0))

h1b_df = h1b_df %>% mutate(data_engineer = ifelse(regexpr('data engineer', tolower(JOB_TITLE)) != -1, 1, 0))

h1b_df = h1b_df %>% mutate(machine_learning = ifelse(regexpr('machine learning', tolower(JOB_TITLE)) != -1, 1, 0))
```

Last part of the data transformation is retaining only the useful columns.

```{r}
h1b_df = h1b_df %>%  select(EMPLOYER_NAME, JOB_TITLE, SOC_NAME, PREVAILING_WAGE, WORKSITE_CITY, WORKSITE_STATE)
```
<h2> Exploratory Data Analysis>
Next, I analyze the prevailing wages for Data Scientist positions.

<h3> Seniority Analysis </h3>




